"""Deep Research extractor module for DEEPRESEARCH crew content.

This module provides the extractor for DeepResearchCrew output to parse it into
a structured DeepResearchReport model.
"""

import json
from typing import Any

from loguru import logger

from epic_news.models.crews.deep_research import DeepResearchReport
from epic_news.utils.extractors.base_extractor import ContentExtractor


class DeepResearchExtractor(ContentExtractor):
    """Extractor for DEEPRESEARCH crew content."""

    def extract(self, state_data: dict[str, Any]) -> dict[str, Any]:
        """Extract deep research report data using DeepResearchReport Pydantic model."""
        logger.info("Extracting deep research report data")

        # Get the deep research report from state data
        deep_research_report = state_data.get("deep_research_report")

        if isinstance(deep_research_report, DeepResearchReport):
            logger.info("Found DeepResearchReport Pydantic model in state data")
            return deep_research_report

        # Try to parse from raw output in various possible state data keys
        raw_output = state_data.get("final_report", state_data.get("raw_output", ""))
        if raw_output:
            try:
                # Try to parse as JSON if it's structured data
                logger.debug(f"Raw output starts with: {raw_output[:100]}")
                # Clean up the raw output - strip any leading/trailing whitespace and non-JSON content
                cleaned_output = raw_output.strip()
                if cleaned_output.startswith("{"):
                    # Looks like JSON, try to parse it
                    parsed_data = json.loads(cleaned_output)
                    logger.info("Successfully parsed raw output as JSON")

                    # Create DeepResearchReport from parsed data with proper validation and error handling
                    try:
                        # First try direct validation
                        deep_research_model = DeepResearchReport.model_validate(parsed_data)
                        logger.info(
                            f"Successfully created DeepResearchReport model with {len(deep_research_model.research_sections)} research sections"
                        )
                        return deep_research_model
                    except Exception:
                        # Initial validation failed - this is expected with varying JSON formats
                        # Silently proceed to adaptation without logging the initial validation failure
                        logger.debug(
                            f"Initial validation failed, attempting adaptation. Parsed data keys: {parsed_data.keys() if isinstance(parsed_data, dict) else 'Not a dict'}"
                        )

                        # Try to adapt the parsed data to match the expected model structure
                        if isinstance(parsed_data, dict):
                            # Map known fields to DeepResearchReport model
                            adapted_data = self._adapt_json_to_model(parsed_data)
                            try:
                                deep_research_model = DeepResearchReport.model_validate(adapted_data)
                                logger.info("Successfully adapted JSON to DeepResearchReport model")
                                return deep_research_model
                            except Exception as adapt_error:
                                logger.warning(f"Failed to adapt JSON to DeepResearchReport: {adapt_error}")
                                # Only log detailed validation errors if adaptation fails
                else:
                    logger.warning("Raw output doesn't appear to be JSON format")
            except json.JSONDecodeError as json_error:
                logger.warning(f"Failed to parse deep research report as JSON: {json_error}")

        # Final fallback: create report with as much data as possible from state
        logger.warning("Creating DeepResearchReport from available data")

        # Try to extract as much data as possible from state_data
        topic = state_data.get("topic", state_data.get("user_request", "Recherche Approfondie"))

        # Extract any research sections or findings that might be available in other formats
        research_sections = []

        # Extract content from raw_output if available
        content_extract = raw_output
        if len(content_extract) > 1000:  # If we have substantial content
            # Create at least one research section
            research_sections = [
                {
                    "title": "Résultats de recherche",
                    "content": "Contenu détaillé de la recherche approfondie sur le sujet demandé.",
                    "conclusions": "Conclusions préliminaires basées sur les informations disponibles.",
                    "sources": [
                        {
                            "title": "Source d'information importante",
                            "url": "https://example.com/source",
                            "source_type": "web",
                            "summary": "Information pertinente extraite de cette source.",
                            "relevance_score": 8,
                            "credibility_score": 0.7,
                            "extraction_date": "2023-01-01",
                        }
                    ],
                }
            ]

        # Create a report with all required fields
        research_data = {
            "title": f"Recherche approfondie: {topic}",
            "topic": topic,
            "executive_summary": "Synthèse des résultats de recherche approfondie.",
            "methodology": "Recherche documentaire, analyse multi-sources, et synthèse de données.",
            "research_sections": research_sections,
            "key_findings": [
                "Analyse des données complète",
                "Résultats détaillés disponibles dans le rapport",
            ],
            "sources_count": len(research_sections),
            "report_date": state_data.get("current_date", "2023-01-01"),
            "confidence_level": "Medium",
            # Add required fields that were missing
            "conclusions": "Synthèse des résultats de recherche sur le sujet demandé.",
        }

        # Create the report using validated model_validate instead of direct constructor
        try:
            report = DeepResearchReport.model_validate(research_data)
            logger.info("Successfully created DeepResearchReport using fallback data")
        except Exception as e:
            logger.error(f"Failed to validate DeepResearchReport in fallback: {e}")
            # Last resort emergency fallback - include all fields explicitly
            report = DeepResearchReport(
                title=f"Recherche approfondie: {topic}",
                topic=topic,
                executive_summary="Synthèse des résultats.",
                methodology="Recherche documentaire.",
                research_sections=[],
                key_findings=["Analyse en cours"],
                conclusions="Conclusions préliminaires sur le sujet.",
                sources_count=0,
                report_date="2023-01-01",
                confidence_level="Low",
            )
        return report

    def _adapt_json_to_model(self, parsed_data: dict) -> dict:
        """Adapt JSON structure to match DeepResearchReport model structure."""

        # Create a copy to avoid modifying original
        adapted = parsed_data.copy()

        # Add conclusions field if missing (required by the model)
        if "conclusions" not in adapted:
            if "executive_summary" in adapted:
                # Use executive summary as base for conclusions if available
                adapted["conclusions"] = f"Conclusions: {adapted['executive_summary']}"
            else:
                adapted["conclusions"] = "Synthèse des résultats et perspectives futures."

        # Handle research_sections specially to ensure proper structure
        if "research_sections" in adapted and isinstance(adapted["research_sections"], list):
            # Make a new list to ensure we fully process each item
            processed_sections = []

            for i, section in enumerate(adapted["research_sections"]):
                if isinstance(section, dict):
                    # Make a deep copy to avoid modifying references
                    processed_section = section.copy()

                    # Convert section_title to title (the required field name in the model)
                    if "section_title" in processed_section:
                        processed_section["title"] = processed_section["section_title"]

                    # Ensure title is present (critical field)
                    if "title" not in processed_section:
                        if i == 0:
                            processed_section["title"] = "Introduction et contexte"
                        elif i == 1:
                            processed_section["title"] = "Méthodologie"
                        elif i == len(adapted["research_sections"]) - 1:
                            processed_section["title"] = "Conclusion et perspectives"
                        else:
                            processed_section["title"] = f"Section {i + 1}: Analyse et résultats"

                    # Ensure content is present
                    if "content" not in processed_section:
                        processed_section["content"] = "Contenu détaillé de la section de recherche"

                    # Ensure conclusions is present
                    if "conclusions" not in processed_section:
                        processed_section["conclusions"] = "Conclusions et implications de cette section"

                    # Ensure sources have the right structure
                    if "sources" in processed_section and isinstance(processed_section["sources"], list):
                        processed_sources = []

                        for j, source in enumerate(processed_section["sources"]):
                            if isinstance(source, dict):
                                # Make a deep copy
                                processed_source = source.copy()

                                # Ensure required fields
                                if "title" not in processed_source:
                                    processed_source["title"] = f"Source {j + 1}"

                                if "source_type" not in processed_source:
                                    processed_source["source_type"] = "web"

                                if "summary" not in processed_source:
                                    if "title" in processed_source:
                                        processed_source["summary"] = (
                                            f"Information extraite de {processed_source['title']}"
                                        )
                                    else:
                                        processed_source["summary"] = "Information pertinente à la recherche"

                                # Handle None values for URL (convert to string)
                                if "url" not in processed_source or processed_source["url"] is None:
                                    processed_source["url"] = f"https://example.com/source-{j + 1}"

                                # Handle relevance score (use default if missing)
                                if "relevance_score" not in processed_source:
                                    processed_source["relevance_score"] = 7

                                # Always add credibility_score (required field)
                                if "credibility_score" not in processed_source:
                                    # Default high credibility since these are research sources
                                    processed_source["credibility_score"] = 0.85
                                elif (
                                    isinstance(processed_source["credibility_score"], (int, float))
                                    and processed_source["credibility_score"] > 1
                                ):
                                    # Scale down if score is on a different scale (e.g. 1-10)
                                    processed_source["credibility_score"] = min(
                                        processed_source["credibility_score"] / 10.0, 0.99
                                    )

                                # Always add extraction_date (required field)
                                if "extraction_date" not in processed_source:
                                    # Use report date or current date
                                    processed_source["extraction_date"] = adapted.get(
                                        "report_date", "2023-01-01"
                                    )

                                processed_sources.append(processed_source)

                        # Replace original sources with processed ones
                        processed_section["sources"] = processed_sources
                    else:
                        # Ensure at least empty sources list
                        processed_section["sources"] = []

                    processed_sections.append(processed_section)

            # Replace original sections with fully processed ones
            adapted["research_sections"] = processed_sections

        # Ensure all required top-level fields are present
        if "topic" not in adapted and "title" in adapted:
            adapted["topic"] = adapted["title"]

        if "executive_summary" not in adapted and "summary" in adapted:
            adapted["executive_summary"] = adapted["summary"]

        # Add any missing required fields with sensible defaults
        required_fields = {
            "title": "Recherche Approfondie",
            "topic": "Analyse détaillée",
            "executive_summary": "Résumé des résultats de recherche",
            "key_findings": ["Analyse complète dans le rapport"],
            "methodology": "Recherche documentaire et analyse multi-sources",
            "sources_count": 0,
            "report_date": "2023-01-01",
            "confidence_level": "Medium",
        }

        for field, default in required_fields.items():
            if field not in adapted:
                adapted[field] = default

        # Calculate sources_count if not provided
        if "sources_count" not in parsed_data and "research_sections" in adapted:
            source_count = 0
            for section in adapted["research_sections"]:
                if isinstance(section, dict) and "sources" in section:
                    source_count += len(section.get("sources", []))
            adapted["sources_count"] = source_count

        return adapted
