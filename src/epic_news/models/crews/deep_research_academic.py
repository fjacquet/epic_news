"""
Mod√®les Pydantic pour le syst√®me Deep Research Acad√©mique
Structures de donn√©es pour communication inter-agents et validation qualit√©
"""

from datetime import datetime
from typing import Optional, Union

from pydantic import BaseModel, Field, validator


class ResearchPlan(BaseModel):
    """Plan de recherche strat√©gique avec m√©thodologie et crit√®res qualit√©"""

    topic: str = Field(..., description="Sujet de recherche principal")
    scope: str = Field(..., description="Port√©e et limites de la recherche")
    objectives: list[str] = Field(..., description="Objectifs de recherche principaux")
    research_questions: list[str] = Field(..., description="Questions de recherche cl√©s")
    methodology: str = Field(..., description="M√©thodologie de recherche d√©taill√©e")
    data_sources: list[str] = Field(..., description="Sources de donn√©es √† explorer")
    quality_criteria: dict[str, float] = Field(..., description="Crit√®res de qualit√© quantitatifs")
    timeline: dict[str, str] = Field(..., description="Calendrier et priorit√©s")
    expected_depth: str = Field(default="PhD", description="Niveau de profondeur attendu")

    @validator("quality_criteria")
    def validate_quality_criteria(cls, v):
        required_criteria = ["min_word_count", "min_sources", "min_statistical_tests"]
        for criterion in required_criteria:
            if criterion not in v:
                raise ValueError(f"Crit√®re qualit√© manquant: {criterion}")
        return v


class CollectedData(BaseModel):
    """Donn√©es collect√©es avec m√©tadonn√©es de qualit√©"""

    source_url: str = Field(..., description="URL de la source")
    source_type: str = Field(..., description="Type de source (web, academic, wikipedia, etc.)")
    credibility_score: float = Field(..., ge=0.0, le=1.0, description="Score de cr√©dibilit√© (0-1)")
    content: str = Field(..., description="Contenu extrait")
    extraction_date: datetime = Field(default_factory=datetime.now)
    key_findings: list[str] = Field(..., description="D√©couvertes cl√©s extraites")
    relevance_score: float = Field(..., ge=0.0, le=1.0, description="Score de pertinence")
    data_quality: str = Field(..., description="√âvaluation qualitative des donn√©es")

    @validator("credibility_score", "relevance_score")
    def validate_scores(cls, v):
        if not 0.0 <= v <= 1.0:
            raise ValueError("Les scores doivent √™tre entre 0.0 et 1.0")
        return v


class QuantitativeAnalysis(BaseModel):
    """Analyse quantitative avec m√©triques statistiques"""

    analysis_type: str = Field(..., description="Type d'analyse (descriptive, corr√©lation, etc.)")
    metrics: dict[str, Union[float, int]] = Field(..., description="M√©triques calcul√©es")
    statistical_tests: list[str] = Field(..., description="Tests statistiques effectu√©s")
    p_values: dict[str, float] = Field(default_factory=dict, description="Valeurs p des tests")
    confidence_intervals: dict[str, list[float]] = Field(
        default_factory=dict, description="Intervalles de confiance"
    )
    effect_sizes: dict[str, float] = Field(default_factory=dict, description="Tailles d'effet")
    visualizations: list[str] = Field(..., description="Graphiques et visualisations g√©n√©r√©s")
    code_executed: str = Field(..., description="Code Python ex√©cut√© pour l'analyse")
    interpretation: str = Field(..., description="Interpr√©tation des r√©sultats statistiques")
    limitations: list[str] = Field(..., description="Limitations de l'analyse")

    @validator("p_values")
    def validate_p_values(cls, v):
        for test, p_val in v.items():
            if not 0.0 <= p_val <= 1.0:
                raise ValueError(f"Valeur p invalide pour {test}: {p_val}")
        return v


class QualityAssessment(BaseModel):
    """√âvaluation qualit√© avec seuils et recommandations"""

    overall_score: float = Field(..., ge=0.0, le=1.0, description="Score qualit√© global")
    word_count: int = Field(..., description="Nombre de mots du rapport")
    source_count: int = Field(..., description="Nombre de sources utilis√©es")
    credible_source_ratio: float = Field(..., ge=0.0, le=1.0, description="Ratio sources cr√©dibles")
    statistical_analysis_present: bool = Field(..., description="Pr√©sence d'analyse statistique")
    academic_structure_score: float = Field(..., ge=0.0, le=1.0, description="Score structure acad√©mique")
    factual_accuracy_score: float = Field(..., ge=0.0, le=1.0, description="Score pr√©cision factuelle")
    coherence_score: float = Field(..., ge=0.0, le=1.0, description="Score coh√©rence logique")

    # Seuils qualit√© PhD
    quality_thresholds: dict[str, Union[int, float]] = Field(
        default={
            "min_word_count": 15000,
            "min_sources": 25,
            "min_credible_ratio": 0.8,
            "min_overall_score": 0.85,
            "min_academic_structure": 0.9,
        }
    )

    meets_phd_standards: bool = Field(default=False, description="Respecte les standards PhD")
    improvement_recommendations: list[str] = Field(..., description="Recommandations d'am√©lioration")
    requires_replanning: bool = Field(default=False, description="N√©cessite re-planification")

    def __init__(self, **data):
        super().__init__(**data)
        self.meets_phd_standards = self._assess_phd_standards()
        self.requires_replanning = not self.meets_phd_standards

    def _assess_phd_standards(self) -> bool:
        """√âvalue si le rapport respecte les standards PhD"""
        thresholds = self.quality_thresholds

        checks = [
            self.word_count >= thresholds["min_word_count"],
            self.source_count >= thresholds["min_sources"],
            self.credible_source_ratio >= thresholds["min_credible_ratio"],
            self.overall_score >= thresholds["min_overall_score"],
            self.academic_structure_score >= thresholds["min_academic_structure"],
            self.statistical_analysis_present,
        ]

        return all(checks)


class ResearchState(BaseModel):
    """√âtat global de la recherche pour orchestration dynamique"""

    research_id: str = Field(..., description="Identifiant unique de la recherche")
    topic: str = Field(..., description="Sujet de recherche")
    current_phase: str = Field(default="planning", description="Phase actuelle")

    # Donn√©es des phases
    research_plan: Optional[ResearchPlan] = None
    collected_data: list[CollectedData] = Field(default_factory=list)
    quantitative_analysis: Optional[QuantitativeAnalysis] = None
    quality_assessment: Optional[QualityAssessment] = None

    # M√©tadonn√©es d'orchestration
    iteration_count: int = Field(default=1, description="Nombre d'it√©rations")
    max_iterations: int = Field(default=3, description="Maximum d'it√©rations autoris√©es")
    phase_history: list[str] = Field(default_factory=list, description="Historique des phases")

    # Statut et contr√¥le qualit√©
    is_complete: bool = Field(default=False, description="Recherche termin√©e")
    needs_replanning: bool = Field(default=False, description="N√©cessite re-planification")
    error_messages: list[str] = Field(default_factory=list, description="Messages d'erreur")

    created_at: datetime = Field(default_factory=datetime.now)
    updated_at: datetime = Field(default_factory=datetime.now)

    def advance_phase(self, next_phase: str):
        """Avance vers la phase suivante"""
        self.phase_history.append(self.current_phase)
        self.current_phase = next_phase
        self.updated_at = datetime.now()

    def trigger_replanning(self, reason: str):
        """D√©clenche une re-planification"""
        self.needs_replanning = True
        self.iteration_count += 1
        self.error_messages.append(f"Re-planification requise: {reason}")
        self.current_phase = "planning"
        self.updated_at = datetime.now()

    def can_continue_iteration(self) -> bool:
        """V√©rifie si une nouvelle it√©ration est possible"""
        return self.iteration_count < self.max_iterations

    def get_completion_status(self) -> dict[str, Union[bool, str, int]]:
        """Retourne le statut de completion"""
        return {
            "is_complete": self.is_complete,
            "current_phase": self.current_phase,
            "iteration": self.iteration_count,
            "meets_standards": self.quality_assessment.meets_phd_standards
            if self.quality_assessment
            else False,
            "word_count": self.quality_assessment.word_count if self.quality_assessment else 0,
            "source_count": len(self.collected_data),
        }


class AcademicReport(BaseModel):
    """Rapport acad√©mique final avec structure PhD"""

    title: str = Field(..., description="Titre du rapport")
    topic: str = Field(..., description="Sujet de recherche")

    # Structure acad√©mique
    executive_summary: str = Field(..., description="R√©sum√© ex√©cutif (2-3 pages)")
    introduction: str = Field(..., description="Introduction et contexte")
    literature_review: str = Field(..., description="Revue de litt√©rature (5-8 pages)")
    methodology: str = Field(..., description="M√©thodologie (2-3 pages)")
    quantitative_analysis: str = Field(..., description="Analyse quantitative (5-10 pages)")
    discussion: str = Field(..., description="Discussion des r√©sultats")
    conclusions: str = Field(..., description="Conclusions et recommandations")
    references: list[str] = Field(..., description="R√©f√©rences bibliographiques")
    appendices: str = Field(default="", description="Annexes")

    # M√©tadonn√©es qualit√©
    word_count: int = Field(..., description="Nombre total de mots")
    source_count: int = Field(..., description="Nombre de sources")
    statistical_tests_count: int = Field(..., description="Nombre de tests statistiques")
    visualization_count: int = Field(..., description="Nombre de visualisations")

    # Validation acad√©mique
    academic_level: str = Field(default="PhD", description="Niveau acad√©mique")
    quality_score: float = Field(..., ge=0.0, le=1.0, description="Score qualit√© global")

    created_at: datetime = Field(default_factory=datetime.now)

    @validator("word_count")
    def validate_word_count(cls, v):
        if v < 15000:  # Minimum PhD standard
            raise ValueError(f"Rapport trop court: {v} mots (minimum 15,000)")
        return v

    def to_html(self) -> str:
        """G√©n√®re le rapport en format HTML acad√©mique"""
        html_content = f"""
        <!DOCTYPE html>
        <html lang="fr">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>{self.title}</title>
            <style>
                body {{ font-family: 'Times New Roman', serif; line-height: 1.6; margin: 40px; }}
                .header {{ text-align: center; margin-bottom: 40px; }}
                .section {{ margin: 30px 0; }}
                .section h2 {{ color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px; }}
                .references {{ font-size: 0.9em; }}
                .metadata {{ background: #f8f9fa; padding: 20px; border-left: 4px solid #3498db; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>üìä {self.title}</h1>
                <p><strong>Recherche Acad√©mique Approfondie</strong></p>
                <p>G√©n√©r√© le {self.created_at.strftime("%d/%m/%Y")}</p>
            </div>
            
            <div class="metadata">
                <h3>üéØ M√©tadonn√©es de Qualit√©</h3>
                <ul>
                    <li><strong>Nombre de mots:</strong> {self.word_count:,}</li>
                    <li><strong>Sources utilis√©es:</strong> {self.source_count}</li>
                    <li><strong>Tests statistiques:</strong> {self.statistical_tests_count}</li>
                    <li><strong>Visualisations:</strong> {self.visualization_count}</li>
                    <li><strong>Score qualit√©:</strong> {self.quality_score:.2%}</li>
                    <li><strong>Niveau acad√©mique:</strong> {self.academic_level}</li>
                </ul>
            </div>
            
            <div class="section">
                <h2>üìã R√©sum√© Ex√©cutif</h2>
                {self.executive_summary}
            </div>
            
            <div class="section">
                <h2>üéØ Introduction</h2>
                {self.introduction}
            </div>
            
            <div class="section">
                <h2>üìö Revue de Litt√©rature</h2>
                {self.literature_review}
            </div>
            
            <div class="section">
                <h2>üî¨ M√©thodologie</h2>
                {self.methodology}
            </div>
            
            <div class="section">
                <h2>üìä Analyse Quantitative</h2>
                {self.quantitative_analysis}
            </div>
            
            <div class="section">
                <h2>üí≠ Discussion</h2>
                {self.discussion}
            </div>
            
            <div class="section">
                <h2>üéØ Conclusions</h2>
                {self.conclusions}
            </div>
            
            <div class="section references">
                <h2>üìñ R√©f√©rences</h2>
                <ol>
                    {"".join(f"<li>{ref}</li>" for ref in self.references)}
                </ol>
            </div>
            
            {f'<div class="section"><h2>üìé Annexes</h2>{self.appendices}</div>' if self.appendices else ""}
        </body>
        </html>
        """
        return html_content
