# Project Roadmap & Task List

This document outlines the development roadmap, current tasks, and a history of completed work for the Epic News project.

## üöÄ Next Up (Priority)

*All priority tasks completed! See Changelog for details.*

## üìù To-Do

### Deep Research System Implementation (Priority)

#### Phase 1: Infrastructure de Base (2-3 jours) ‚úÖ TERMIN√â

- [x] **Mod√®les Pydantic DeepResearch:** ‚úÖ Cr√©√© les mod√®les `ResearchSource`, `ResearchSection`, `QuantitativeAnalysis`, et `DeepResearchReport` dans `src/epic_news/models/crews/deep_research.py`
- [x] **Extension ContentState:** ‚úÖ Confirm√© que le champ `deep_research_report` existe d√©j√† pour supporter DeepResearchReport
- [x] **DeepResearchExtractor:** ‚úÖ Cr√©√© l'extracteur dans `src/epic_news/utils/content_extractors.py` et enregistr√© dans ContentExtractorFactory
- [x] **Template HTML:** ‚úÖ Confirm√© que DeepResearchRenderer existe et est int√©gr√© dans RendererFactory pour TemplateManager
- [x] **Flow Integration:** ‚úÖ Modernis√© `generate_deep_research()` dans main.py avec ContentExtractorFactory et TemplateManager

#### Phase 2: Agents et Configuration (3-4 jours) ‚úÖ TERMIN√â

- [x] **Configuration YAML Compl√®te:** ‚úÖ Confirm√© `agents.yaml` avec 4 agents sp√©cialis√©s (primary_researcher, wikipedia_specialist, content_analyst, report_generator)
- [x] **T√¢ches Structur√©es:** ‚úÖ Confirm√© `tasks.yaml` avec `output_pydantic=DeepResearchReport` pour la t√¢che finale
- [x] **Outils Sp√©cialis√©s:** ‚úÖ Int√©gr√© ScrapeNinjaTool, SerperDevTool, WikipediaSearchTool, WikipediaArticleTool dans DeepResearchCrew
- [x] **R√¥les Fran√ßais:** ‚úÖ Confirm√© backstories, goals, et r√¥les professionnels en fran√ßais pour chaque agent
- [x] **Sequential Process:** ‚úÖ Configur√© `Process.sequential` avec async_execution pour optimisation des performances
- [x] **Tests d'Int√©gration:** ‚úÖ Valid√© avec script de test complet - 4 agents, 4 t√¢ches, g√©n√©ration HTML (10,808 caract√®res)

#### Phase 3: REFONTE ACAD√âMIQUE DEEP RESEARCH (5-7 jours) - CRITIQUE ‚ö†Ô∏è REFONTE COMPL√àTE

**ANALYSE YAML TERMIN√âE:** Architecture 6-agents correcte MAIS √©carts critiques identifi√©s vs sp√©cifications acad√©miques PhD

**√âCARTS CRITIQUES IDENTIFI√âS:**

- ‚ùå **Code Interpreter manquant:** Agent data_analyst sans outils quantitatifs (OBLIGATOIRE pour analyse statistique)
- ‚ùå **Pas de CrewAI Flow:** Utilise crew s√©quentiel basique vs orchestration dynamique (@listen/@router)
- ‚ùå **Pas de mod√®les Pydantic:** Communication inter-agents par texte vs donn√©es structur√©es
- ‚ùå **Validation qualit√© insuffisante:** Pas de seuils quantitatifs ni re-planification adaptative
- ‚ùå **Rapports superficiels:** 5k caract√®res vs 20+ pages acad√©miques requises

**PLAN DE REFONTE ACAD√âMIQUE:**

##### Phase 3A: Infrastructure Critique (2-3 jours) ‚úÖ TERMIN√â

- [x] **Analyse YAML Compl√®te:** ‚úÖ Confirm√© architecture 6-agents mais √©carts fonctionnels critiques
- [x] **Code Interpreter Integration:** ‚úÖ Ajout√© allow_code_execution=True √† data_analyst (Docker install√©, fonctionnel)
- [x] **Mod√®les Pydantic Acad√©miques:** ‚úÖ Cr√©√© ResearchPlan, CollectedData, QuantitativeAnalysis, QualityAssessment, ResearchState, AcademicReport dans `deep_research_academic.py`
- [x] **CrewAI Flow Dynamique:** ‚úÖ Impl√©ment√© @start(), @listen(), @router() dans DeepResearchFlowAcademicFixed et DeepResearchFlowPhD

##### Phase 3B: Standards Acad√©miques (3-4 jours) üöÄ EN COURS

- [ ] **Structure Rapport Acad√©mique:** Impl√©menter sections obligatoires compl√®tes (r√©sum√© ex√©cutif, revue litt√©rature, m√©thodologie, analyse quantitative, conclusions)
- [ ] **Validation Qualit√© Rigoureuse:** Renforcer seuils quantitatifs (15k+ mots, 30+ sources, analyses statistiques avec Code Interpreter)
- [ ] **Boucles It√©ratives Avanc√©es:** Am√©liorer re-planification automatique et validation multi-crit√®res
- [ ] **Tests Acad√©miques Approfondis:** Validation end-to-end avec sujets complexes et m√©triques PhD
- [ ] **Optimisation Longueur:** Atteindre consistamment 15,000+ mots vs 750 mots actuels
- [ ] **Analyse Quantitative R√©elle:** Exploiter pleinement Code Interpreter pour calculs statistiques, visualisations, tests d'hypoth√®ses

#### Phase 4: CrewAI Code Interpreter (3-4 jours) - CRITIQUE URGENT

**MANQUE CRITIQUE:** Aucune analyse quantitative/statistique dans le rapport actuel (exigence PhD)

- [ ] **Configuration Docker:** Configurer environnement Docker pour ex√©cution de code s√©curis√©e
- [ ] **Int√©gration Analyste:** Ajouter Code Interpreter √† l'agent Analyste de Donn√©es (OBLIGATOIRE pour deep research)
- [ ] **D√©pendances Python:** Configurer pandas, numpy, scipy, matplotlib, seaborn dans l'environnement
- [ ] **Scripts d'Analyse:** D√©velopper templates d'analyse statistique (corr√©lations, tendances, visualisations)
- [ ] **S√©curit√©:** Impl√©menter isolation et validation des inputs pour ex√©cution de code
- [ ] **Tests Quantitatifs:** Valider capacit√©s d'analyse statistique et g√©n√©ration de graphiques
- [ ] **Int√©gration Rapport:** Assurer que les analyses quantitatives sont int√©gr√©es dans le rapport final HTML

#### Phase 5: Validation Acad√©mique et Tests (3-4 jours) - STANDARDS PhD

**OBJECTIF:** Atteindre niveau acad√©mique PhD avec rapports de dizaines de pages

- [ ] **Tests End-to-End Acad√©miques:** Valider g√©n√©ration de rapports de 20+ pages avec structure acad√©mique compl√®te
- [ ] **Validation M√©thodologique:** Tester planification ‚Üí collecte ‚Üí analyse ‚Üí synth√®se ‚Üí validation
- [ ] **Standards Acad√©miques:** V√©rifier format PhD (r√©sum√© ex√©cutif, m√©thodologie, analyse quantitative, conclusions, r√©f√©rences)
- [ ] **Validation Fran√ßaise:** Tester qualit√© linguistique et standards acad√©miques fran√ßais
- [ ] **Tests Quantitatifs:** Valider analyses statistiques, graphiques, et interpr√©tations
- [ ] **Tests de Performance:** Optimiser pour analyses quantitatives lourdes et rapports volumineux
- [ ] **Tests de R√©gression:** V√©rifier compatibilit√© avec crews existants
- [ ] **Benchmarking Qualit√©:** Comparer avec standards de recherche acad√©mique r√©els
- [ ] **Documentation:** Mettre √† jour guides utilisateur et documentation technique

#### Phase 6: Refonte Critique Deep Research (URGENT - 5-7 jours)

**STATUT ACTUEL:** ‚ùå √âCHEC - Rapport g√©n√©r√© = r√©sum√© simpliste (5k caract√®res) vs exigences PhD

**ACTIONS IMM√âDIATES:**

- [ ] **Audit Complet:** Analyser √©carts entre impl√©mentation actuelle et sp√©cifications docs/99_deep_research.md
- [ ] **Refonte Architecture:** Remplacer flux v2 simpliste par architecture acad√©mique compl√®te
- [ ] **Agents Sp√©cialis√©s:** Impl√©menter les 6 agents selon sp√©cification (Strat√®ge, Collecteur, Wikipedia, Analyste+Code, R√©dacteur, QA)
- [ ] **Orchestration Dynamique:** Cr√©er flux CrewAI avec @listen/@router pour re-planification adaptative
- [ ] **Analyse Quantitative:** Int√©grer Code Interpreter pour analyses statistiques obligatoires
- [ ] **Format Acad√©mique:** D√©velopper templates HTML pour rapports de niveau PhD (20+ pages)
- [ ] **Validation Qualit√©:** Impl√©menter boucles de validation et am√©lioration continue
- [ ] **Test R√©el:** Valider avec sujet complexe et mesurer qualit√© acad√©mique vs standards PhD

**CRIT√àRES DE SUCC√àS:**

- ‚úÖ Rapport de 20+ pages avec structure acad√©mique compl√®te
- ‚úÖ Analyse quantitative avec graphiques et statistiques
- ‚úÖ M√©thodologie rigoureuse multi-√©tapes document√©e
- ‚úÖ Re-planification dynamique et validation qualit√©
- ‚úÖ Format HTML professionnel niveau publication acad√©mique

### Code Cleanup and Maintenance

- [ ] **Remove Unused and Deprecated Code:** Systematically review the `src/` directory to remove commented-out code blocks, unused imports, and other dead code to improve readability and maintainability.
  - **Example File:** `src/epic_news/crews/company_news/company_news_crew.py`

### Containerization

- [ ] **Verify CrewAI Execution in Docker:** Finalize and verify the setup for running CrewAI reliably within a containerized environment.

## üìã Backlog (Future Tasks)

- [ ] **n8n Integration:**
  - [ ] Connect n8n to retrieve data initiated by CrewAI.
  - [ ] Connect n8n to initiate CrewAI workflows.
- [ ] **Technology Watch Agents:** Create new agents for monitoring and reporting on various technologies (Nutanix, Netbackup, Commvault, etc.).
- [ ] **Free API Integration:** Research, evaluate, and integrate valuable free APIs to expand the project's data-gathering capabilities.

## ‚úÖ Changelog (Completed Tasks)

### Q3 2025

- **DeepResearchCrew Integration (July 2025):**

- **Menu Designer Integration & Recipe Generation (July 2025):**
  - **End-to-End Workflow:** Successfully implemented complete menu planning to recipe generation integration with 30 recipes generated from weekly menu plans.
  - **Menu Parser Fix:** Updated `MenuGenerator.parse_menu_structure()` to handle new 'dishes' array format while maintaining backward compatibility with old starter/main_course/dessert structure.
  - **Recipe Generation:** Fixed CookingCrew template variable issues (patrika_file, output_file) enabling generation of both JSON and YAML recipe formats.
  - **HTML Rendering:** Fixed MenuRenderer to properly extract and display actual dish names from JSON menu structure instead of generic placeholders.
  - **Production Ready:** Complete integration now processes all dishes from validated weekly menu plans and generates individual recipes with proper file naming.

- **Comprehensive Test Quality Assurance (July 2025):**
  - **Test Suite Excellence:** Achieved 461 tests passing with only 5 skipped, zero failures across the entire project.
  - **Coverage & Quality:** Generated comprehensive test coverage reports and maintained high code quality standards.
  - **Bug Documentation:** Created detailed bug reports and systematic QA processes following project development principles.
  - **Test Infrastructure:** Enhanced test reliability with proper fixtures, mocking, and cross-platform compatibility.
  - **Lint & Format:** All code passes Ruff formatting and linting checks with zero issues.

- **Project Structure & Code Quality Improvements (Q3 2025):**
  - **ContentState Model Refactoring:** Replaced `Union[Any, None]` fields with specific Pydantic models for improved type safety and data clarity.
  - **Pydantic Standardization:** Consistently used `Optional[X]` instead of `Union[X, None]` across all models for better compatibility.
  - **Script Organization:** Relocated helper scripts from `src/epic_news/bin` to top-level `scripts/` directory for better separation of concerns.
  - **Logging Consistency:** Standardized `app.py` to use `loguru` for consistent logging across the entire application.

- **Complete HTML Rendering Integration (Q3 2025):**
  - **OSINT Crews Integration:** Successfully connected HTML rendering pipeline for all 9 OSINT and specialized crews.
  - **Crew Coverage:** Integrated `company_profiler`, `cross_reference_report_crew`, `geospatial_analysis`, `hr_intelligence`, `legal_analysis`, `menu_designer`, `sales_prospecting`, `tech_stack`, and `web_presence`.
  - **Unified Reporting:** All crews now generate consistent, professional HTML reports with proper styling and data presentation.

- **Pydantic Model & OSINT Crew Refactoring:**
  - **Structured Outputs:** Refactored 8 OSINT crews (`CompanyProfiler`, `CrossReferenceReport`, `GeospatialAnalysis`, `HRIntelligence`, `LegalAnalysis`, `WebPresence`, `TechStack`) to use Pydantic models for structured, reliable outputs.
  - **Centralized Models:** Relocated all crew-specific Pydantic models to a dedicated `src/epic_news/models/crews/` directory, improving project structure and modularity.
  - **Comprehensive Testing:** Created and validated unit tests for all new and relocated models to ensure data integrity.
  - **Project-Wide Updates:** Updated all import paths across the project to reflect the new model locations and fixed all resulting test failures.

- **HTML Reporting System:**
  - **Cross-Reference Report:** Implemented a complete HTML report generation pipeline for the Cross-Reference Crew, including an HTML factory, a Jinja2 template, and a dynamic regeneration script.
  - **Main Integration:** Integrated the HTML generation logic into the main application entry point.

- **Logging Modernization:**
  - **Loguru Integration:** Replaced the standard `logging` module with `loguru` across the core application for more powerful and configurable logging.
  - **Configuration:** Set up `loguru` sinks for both console and file-based logging.
  - **Documentation:** Updated the development guide to reflect the new logging standards.

- **Advanced Testing Libraries:**
  - **Dependencies:** Integrated `Faker`, `pytest-mock`, and `pendulum` to enhance the testing framework with realistic data generation, mocking, and precise time control.
  - **Standards:** Created sample tests and updated the development guide to establish new testing best practices.

- **Initial Dockerization Setup:**
  - **Dockerfiles:** Created `Dockerfile.api` and `Dockerfile.streamlit` to containerize the application components.
  - **Compose Files:** Set up `docker-compose.yml` files for orchestrating the development and production environments.

- **HTML Rendering for Core Crews:**
  - **Integrated 16 crews** with the HTML rendering pipeline, including `classify`, `company_news`, `cooking`, `fin_daily`, `holiday_planner`, and more.
